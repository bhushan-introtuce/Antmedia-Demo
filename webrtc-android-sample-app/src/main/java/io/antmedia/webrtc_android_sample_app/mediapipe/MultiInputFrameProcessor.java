package io.antmedia.webrtc_android_sample_app.mediapipe;

import android.content.Context;
import android.util.Log;

import com.google.mediapipe.components.FrameProcessor;
import com.google.mediapipe.framework.AppTextureFrame;
import com.google.mediapipe.framework.MediaPipeException;
import com.google.mediapipe.framework.Packet;

public class MultiInputFrameProcessor extends FrameProcessor {
    static private String TAG = "FRAME-PROCESSOR";
    private String bgVideoInputStream;
    private boolean switchSeg = false;

    /**
     * Constructor.
     *
     * @param context             an Android {@link Context}.
     * @param parentNativeContext a native handle to a GL context. The GL context(s) used by the
     *                            calculators in the graph will join the parent context's sharegroup, so that textures
     *                            generated by the calculators are available in the parent context, and vice versa.
     * @param graphName           the name of the file containing the binary representation of the graph.
     * @param inputStream         the graph input stream that will receive input video frames to be segmented.
     * @param outputStream        the output stream from which output frames will be produced.
     * @param bgVideoInputStream  the graph input stream that will receive background frames to be overlay.
     */
    public MultiInputFrameProcessor(Context context, long parentNativeContext, String graphName,
                                    String inputStream, String outputStream, String bgVideoInputStream) {
        super(context, parentNativeContext, graphName, inputStream, outputStream);
        this.bgVideoInputStream = bgVideoInputStream;
    }

    long bgTimestamp;
    long fgTimestamp;

    public long getBgTimestamp() {
        return bgTimestamp;
    }
    public long getFgTimestamp() {
        return fgTimestamp;
    }

    public void onNewFrame(AppTextureFrame foreground, AppTextureFrame background) {
        Packet fgPacket = null;
        Packet bgPacket = null;

        try {
            if (Log.isLoggable(TAG, Log.VERBOSE)) {
                Log.v(TAG, String.format("Foreground tex: %d width: %d height: %d",
                        foreground.getTextureName(), foreground.getWidth(), foreground.getHeight()));
                Log.v(TAG, String.format("Background tex: %d width: %d height: %d",
                        background.getTextureName(), background.getWidth(), background.getHeight()));
            }

            if (!maybeAcceptNewFrame(foreground.getTimestamp())) {
                foreground.release();
                background.release();
                return;
            }

            if (addFrameListener != null) {
                addFrameListener.onWillAddFrame(foreground.getTimestamp());
                addFrameListener.onWillAddFrame(background.getTimestamp());
            }

            fgPacket = packetCreator.createGpuBuffer(foreground);
            bgPacket = packetCreator.createGpuBuffer(background);

            fgTimestamp = foreground.getTimestamp();
            bgTimestamp = background.getTimestamp();

            foreground = null; // fgPacket takes ownership of frame and will release it.
            background = null; // bgPacket takes ownership of frame and will release it.

            try {
                // addConsumablePacketToInputStream allows the graph to take exclusive ownership of the
                // packet, which may allow for more memory optimizations.
                if (!switchSeg) {
                    mediapipeGraph.addConsumablePacketToInputStream(
                            bgVideoInputStream, fgPacket, fgTimestamp);
                    fgPacket = null;
                    if (bgPacket!=null) {
                        mediapipeGraph.addConsumablePacketToInputStream(videoInputStream, bgPacket,
                                fgTimestamp);
                        bgPacket = null;
                    }
                }
                else {
                    mediapipeGraph.addConsumablePacketToInputStream(
                            videoInputStream , fgPacket, fgTimestamp);
                    fgPacket = null;
                    if (bgPacket!=null) {
                        mediapipeGraph.addConsumablePacketToInputStream(bgVideoInputStream,
                                bgPacket, fgTimestamp);
                        bgPacket = null;
                    }
                }

            } catch (MediaPipeException e) {
                if (asyncErrorListener == null) {
                    Log.e(TAG, "Mediapipe error: ", e);
                } else {
                    throw e;
                }
            }

            if(fgPacket!=null)
            fgPacket.release();

            if(bgPacket!=null){
                bgPacket.release();
            }
        } catch (RuntimeException e) {
            if (asyncErrorListener != null) {
                asyncErrorListener.onError(e);
            } else {
                throw e;
            }
        } finally {
            if (fgPacket != null) {
                fgPacket.release();
            }
            if (bgPacket != null) {
                bgPacket.release();
            }
            if (foreground != null) {
                foreground.release();
            }
            if (background != null) {
                background.release();
            }
        }

    }
}